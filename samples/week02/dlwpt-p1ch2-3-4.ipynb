{"cells":[{"cell_type":"markdown","source":["This post implements the examples and exercises in the book \"[Deep Learning with Pytorch](https://pytorch.org/deep-learning-with-pytorch)\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n","\n","It is based on [this repo](https://github.com/qutang/jupyter_notebook_articles) by Jutang\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digitalideation/digcre_h2201/blob/master/samples/week02/dlwpt-p1ch2-3-4.ipynb)"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00001-81a8f5da-06d0-46a4-86fe-412e145fbfd3","output_cleared":false}},{"cell_type":"markdown","source":["## Chapter I: Setting up pytorch environment\n","\n","End-to-end data flow pipeline of deep learning applications.\n","\n","> ![c1_diagram](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c1.png)\n","> \n","> -- From the book \"Deep Learning with Pytorch\"\n","\n"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00002-691c8165-6775-4139-b454-72ef22a81d8f","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["import os\n","import torch\n","torch.cuda.is_available()"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00003-86f673eb-d251-4bc7-b1dc-7b4868d94bed","output_cleared":false,"source_hash":"e847cbf9","execution_millis":4,"execution_start":1606271017699}},{"cell_type":"markdown","source":["## Chapter II: Using pretrained models\n","\n","It is only about the inference process when we only use the pretrained deep learning models. I think it is actually a better way to step into deep learning algorithms for novices because it draws users' attention on what the network can achieve at first and it also separates the concern of data preprocessing from the training process, which both could be tedious and complicated.\n","\n","The inference process of image classification using deep learning\n","\n","> ![dl_pytorch_c2_1](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c2_1.png)\n","> \n","> -- From the book \"Deep Learning with Pytorch\""],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00004-a92d3c32-bc84-48dc-8cf3-71dfa9848f7e","output_cleared":false}},{"cell_type":"markdown","source":["### Image classifications with AlexNet and ResNet101"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00005-062bcc19-4410-4dc1-83cb-6a3131f132c7","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["import torchvision as vis"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00006-e31bff3c-966b-47b9-82cf-9a887cfe6fe1","output_cleared":false,"source_hash":"9594fa02","execution_millis":151,"execution_start":1606271017704}},{"cell_type":"code","execution_count":null,"source":["vis_models = vis.models\n","dir(vis_models)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00007-b20b5a0b-f1f8-4483-b6f6-3f1b553acd47","output_cleared":false,"source_hash":"e1739cf2","execution_millis":7,"execution_start":1606271017860}},{"cell_type":"code","execution_count":null,"source":["# AlexNet, untrained\n","alex_net = vis_models.AlexNet()\n","# AlexNet, pretrained\n","alex_net = vis_models.alexnet(pretrained=True)\n","# ResNet101, pretrained\n","res_net_101 = vis_models.resnet101(pretrained=True)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00008-6f5e4821-85aa-45e9-a4ae-ddf15a19fbc4","output_cleared":false,"source_hash":"c0913a5e","execution_millis":4310,"execution_start":1606271017871}},{"cell_type":"code","execution_count":null,"source":["# Check network structure\n","alex_net"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00009-1ebf0b78-6b88-4022-a5ab-4c8082029e79","output_cleared":false,"source_hash":"531a204e","execution_millis":2,"execution_start":1606271022184}},{"cell_type":"markdown","source":["The network structure of __AlexNet__\n","\n","> ![dl_pytorch_c2_2_alexnet](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c2_2_alexnet.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\""],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00010-c0078781-15f6-4d7f-b05e-4afb183b37f8","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Define preprocess transforms for input images\n","preprocess = vis.transforms.Compose([\n","    vis.transforms.Resize(256),\n","    vis.transforms.CenterCrop(224),\n","    vis.transforms.ToTensor(),\n","    vis.transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00011-2eecdc28-6cb1-454f-b79e-637474f2efde","output_cleared":false,"source_hash":"f488ade1","execution_millis":0,"execution_start":1606271022211}},{"cell_type":"code","execution_count":null,"source":["# Preprocess a test image\n","import PIL as pil\n","import requests\n","from io import BytesIO\n","\n","img_url = 'https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dog.jpg'\n","response = requests.get(img_url)\n","img = pil.Image.open(BytesIO(response.content))\n","\n","img_t = preprocess(img)\n","img"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-b9f7a484-5334-4bee-86e3-11116405effb","output_cleared":false,"source_hash":"98fe9dc9","execution_millis":154,"execution_start":1606271022218}},{"cell_type":"code","execution_count":null,"source":["# Transform it into input batches\n","batch_t = torch.unsqueeze(img_t, 0)\n","batch_t.size()"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00013-9eb122d1-4ad5-455f-a33c-af216d8366d4","output_cleared":false,"source_hash":"291e0c2c","execution_millis":1,"execution_start":1606271022413}},{"cell_type":"code","execution_count":null,"source":["# Load ImageNet classes\n","class_url = \"https://gist.githubusercontent.com/ageitgey/4e1342c10a71981d0b491e1b8227328b/raw/24d78ea09a31fdff540a8494886e0051e3ad68f8/imagenet_classes.txt\"\n","response = requests.get(class_url)\n","content = BytesIO(response.content).getvalue().decode(\"utf-8\")\n","lines = content.split('\\n')[4:]\n","labels = [line.split(',')[1].strip() for line in lines]\n","print(labels)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00014-f85f15bf-5e9c-4621-aa41-cf2ff88bd56a","output_cleared":false,"source_hash":"7f3ca78e","execution_millis":120,"execution_start":1606271022414}},{"cell_type":"code","execution_count":null,"source":["# Run pretrained AlexNet on the test image and produce top 5 predictions\n","alex_net.eval()\n","out = alex_net(batch_t)\n","out_perc = torch.nn.functional.softmax(out, dim=1)[0] * 100\n","\n","sorted_scores, sorted_indices = torch.sort(out, descending=True)\n","\n","[(labels[i], out_perc[i].item()) for i in sorted_indices[0][:5]]"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00015-039ec70f-5737-4264-8f9f-33af76e11410","output_cleared":false,"source_hash":"c23d9b40","execution_millis":192,"execution_start":1606271022545}},{"cell_type":"code","execution_count":null,"source":["# Run pretrained ResNet101 on the test image and produce top 5 predictions\n","res_net_101.eval()\n","out = res_net_101(batch_t)\n","out_perc = torch.nn.functional.softmax(out, dim=1)[0] * 100\n","\n","sorted_scores, sorted_indices = torch.sort(out, descending=True)\n","\n","[(labels[i], out_perc[i].item()) for i in sorted_indices[0][:5]]"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00016-1639c8fc-c68c-49b3-a0e2-68d9a5e4d853","output_cleared":false,"source_hash":"3a703e2a","execution_millis":480,"execution_start":1606271022737}},{"cell_type":"markdown","source":["### Generating fake real-looking images using GAN networks (CycleGAN)\n","\n","GAN game is an adversial process that two networks are competing each other in a cheating game, where the \"generator\" network is the cheater while the \"discriminator\" network is the wiseman. When succeeding, the \"generator\" network can produce real-looking images that the \"discriminator\" network cannot discern.\n","\n","The process of GAN game\n","\n","> ![dl_pytorch_c2_3_gan](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c2_3_gan.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\""],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00017-0643c0e9-19c9-45af-8acf-0c2e6f8a45ee","output_cleared":false}},{"cell_type":"markdown","source":["#### CycleGAN\n","\n","Different from normal GAN network, CycleGAN can learn to generate fake images in a different domain using training data from two different domains.\n","\n","The process of CycleGAN game\n","\n","> ![](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c2_4_cyclegan.png)\n",">\n","> -- From the book \"Deep Learning with PyTorch\"\n","\n","---\n"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00018-3f72d1b0-281d-4102-bbe5-6fec2c938712","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# The implementation of CycleGAN generator using ResNet\n","# See https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch2/3_cyclegan.ipynb\n","import torch.nn as nn\n","\n","class ResNetBlock(nn.Module): # <1>\n","\n","    def __init__(self, dim):\n","        super(ResNetBlock, self).__init__()\n","        self.conv_block = self.build_conv_block(dim)\n","\n","    def build_conv_block(self, dim):\n","        conv_block = []\n","\n","        conv_block += [nn.ReflectionPad2d(1)]\n","\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n","                       nn.InstanceNorm2d(dim),\n","                       nn.ReLU(True)]\n","\n","        conv_block += [nn.ReflectionPad2d(1)]\n","\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n","                       nn.InstanceNorm2d(dim)]\n","\n","        return nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x) # <2>\n","        return out\n","\n","\n","class ResNetGenerator(nn.Module):\n","\n","    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): # <3> \n","\n","        assert(n_blocks >= 0)\n","        super(ResNetGenerator, self).__init__()\n","\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.ngf = ngf\n","\n","        model = [nn.ReflectionPad2d(3),\n","                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n","                 nn.InstanceNorm2d(ngf),\n","                 nn.ReLU(True)]\n","\n","        n_downsampling = 2\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n","                                stride=2, padding=1, bias=True),\n","                      nn.InstanceNorm2d(ngf * mult * 2),\n","                      nn.ReLU(True)]\n","\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            model += [ResNetBlock(ngf * mult)]\n","\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n","                                         kernel_size=3, stride=2,\n","                                         padding=1, output_padding=1,\n","                                         bias=True),\n","                      nn.InstanceNorm2d(int(ngf * mult / 2)),\n","                      nn.ReLU(True)]\n","\n","        model += [nn.ReflectionPad2d(3)]\n","        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n","        model += [nn.Tanh()]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input): # <3>\n","        return self.model(input)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00019-dcf8a317-b1ae-47b3-8e9e-e16491857f08","output_cleared":false,"source_hash":"2b8f69a7","execution_millis":8,"execution_start":1606271023229}},{"cell_type":"code","execution_count":null,"source":["# The structure of the generator is complicated.\n","\n","netG = ResNetGenerator()\n","netG"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00020-f4a6c328-72ef-4f77-b01f-2c1def61f93d","output_cleared":false,"source_hash":"a1a21d00","execution_millis":106,"execution_start":1606271023241}},{"cell_type":"code","execution_count":null,"source":["# Load pretrained horse-to-zebra CycleGAN model parameters\n","# See https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/data/p1ch2\n","\n","model_url = 'https://github.com/deep-learning-with-pytorch/dlwpt-code/raw/master/data/p1ch2/horse2zebra_0.4.0.pth'\n","response = requests.get(model_url)\n","model_data = torch.load(BytesIO(response.content))\n","netG.load_state_dict(model_data)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00021-e2b93af8-ce09-4f96-90aa-8c409ee31f3f","output_cleared":false,"source_hash":"613ba1c1","execution_millis":945,"execution_start":1606271023351}},{"cell_type":"code","execution_count":null,"source":["# Again, prepare the preprocessing transforms for input images\n","# A simple resize is good enough\n","\n","preprocess = vis.transforms.Compose([\n","    vis.transforms.Resize(256),\n","    vis.transforms.ToTensor()\n","])"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00022-d65d43fa-5fe4-4be0-875e-9523a07d72a4","output_cleared":false,"source_hash":"f6ea8033","execution_millis":3,"execution_start":1606271024301}},{"cell_type":"code","execution_count":null,"source":["# Load our test images we downloaded\n","\n","img_url = 'https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/horse.jpg'\n","response = requests.get(img_url)\n","img = pil.Image.open(BytesIO(response.content))\n","img"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00023-c191bc85-a811-4f93-9553-9d600e68e942","output_cleared":false,"source_hash":"254bd60d","execution_millis":1609,"execution_start":1606271024307}},{"cell_type":"code","execution_count":null,"source":["# Prepare the image as network input\n","img_t = preprocess(img)\n","batch_t = torch.unsqueeze(img_t, 0)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00024-5f9337dc-bc2a-463a-ba16-ff32fcfb00e0","output_cleared":false,"source_hash":"e2348389","execution_millis":13,"execution_start":1606271025918}},{"cell_type":"code","execution_count":null,"source":["# Run the pretrained generator on the image\n","netG.eval()\n","batch_out = netG(batch_t)\n","\n","# Transform the output back to an image\n","out_t = (batch_out.data.squeeze() + 1.0) / 2.0\n","out_img = vis.transforms.ToPILImage()(out_t)\n","out_img"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00025-8db5b5ee-50b3-4d2e-a3e5-05ce0d23a3ef","output_cleared":false,"source_hash":"f99e09ad","execution_millis":2874,"execution_start":1606271025919}},{"cell_type":"markdown","source":["### Generating scene description using captioning model\n","\n","Unlike the CNN model used for image classification and GAN model used for image generation, the captioning model uses RNN (Recurrent neural network) model for text generation, prepended with a CNN model for image classification.\n","\n","\"Recurrent\" means the output of previous forward pass will serve as the input of the current forward pass.\n","\n","Concept of a captioning model\n","\n","> ![dl_pytorch_c2_5_caption](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c2_5_caption.png)\n",">\n","> -- From the Book \"Deep Learning with Pytorch\"\n"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00026-3d615870-935c-4b7b-8964-5fe6453f201e","output_cleared":false}},{"cell_type":"markdown","source":["#### The NeuralTalk2 captioning model\n","\n","The model[^1] was proposed by Andrea Karpathy. Here we use the implementation[^2] provided by the book, with mostly the default settings. Please visit the repo to see full available options. Qutang's forked repo[^3] applied two fixes to the original code.\n","\n","1. Fixed the error when loading the infos pickle file on Windows.\n","2. Support `~` in input file paths.\n","\n","[^1]: Andrej Karpathy and Li Fei-Fei, “Deep Visual-Semantic Alignments for Generating Image Descriptions,”\n","https://cs.stanford.edu/people/karpathy/cvpr2015.pdf.\n","[^2]: https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch\n","[^3]: https://github.com/qutang/ImageCaptioning.pytorch\n","\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00027-bde2e687-3cdb-4552-ba18-f41db4f1b78e","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# run the pretrained model, it will run the two image examples we used before and also the zebra image we generated using the CycleGAN model\n","!cd ../ImageCaption.pytorch && python eval.py --model ./data/FC/fc-model.pth --infos_path ./data/FC/fc-infos.pkl --image_folder ../jupyter_notebook_articles/images/"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00028-0b390374-b184-401b-907b-6c2f2509a364","output_cleared":false,"source_hash":"7c655745","execution_millis":7528,"execution_start":1606272258134}},{"cell_type":"code","execution_count":null,"source":["# Display the results\n","import json\n","from IPython.display import display\n","with open('../ImageCaption.pytorch/vis/vis.json', mode='r') as f:\n","    result = json.load(f)\n","img_files = [os.path.join('../jupyter_notebook_articles/images/', name) for name in os.listdir('../jupyter_notebook_articles/images/')]\n","\n","for caption, img_file in zip(result, img_files):\n","    img = pil.Image.open(img_file)\n","    display(img)\n","    print(caption['caption'])"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00029-efd9c3e1-e85f-4b30-b91c-585ad66b1173","output_cleared":false,"source_hash":"81362365","execution_millis":1,"execution_start":1606272284150}},{"cell_type":"markdown","source":["### Use models via pytorch hub\n","\n","Here we use the RoBERTa transformer model[^4] (based on BERT) as an example.\n","\n","[^4]: https://pytorch.org/hub/pytorch_fairseq_roberta/\n","\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00030-56afe14e-f17b-4b7b-b30a-f7026b3c9e63","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Import the BERT transformer model using pytorch hub\n","import torch\n","roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')"],"outputs":[],"metadata":{"deepnote_cell_type":"code","scrolled":true,"tags":[],"cell_id":"00031-186fbc30-e143-4a27-bba8-e245d6fe2513","output_cleared":false,"source_hash":"b7d722df","execution_millis":26614,"execution_start":1606272548079}},{"cell_type":"code","execution_count":null,"source":["# Use the roberta model to transform a sentence to vector representation\n","tokens = roberta.encode('I am running!')\n","print(tokens.tolist())\n","# The vector representation can be converted back to the text\n","roberta.decode(tokens)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00032-dc1ad3b0-8d59-4eb6-89d0-a788cfbca782","output_cleared":false,"source_hash":"149cd530","execution_millis":1,"execution_start":1606272614217}},{"cell_type":"code","execution_count":null,"source":["tokens2 = roberta.encode('I am walking!')\n","print(tokens2.tolist())\n","# The vector representation can be converted back to the text\n","roberta.decode(tokens2)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00033-39785211-5296-4a4f-8c7c-6ded6bd74b26","output_cleared":false,"source_hash":"abd5d677","execution_millis":4}},{"cell_type":"code","execution_count":null,"source":["tokens3 = roberta.encode('I run very fast!')\n","print(tokens3.tolist())\n","# The vector representation can be converted back to the text\n","roberta.decode(tokens3)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00034-889a0895-e647-4f70-8f10-26679153e72b","output_cleared":false,"source_hash":"4c2075d1","execution_millis":2}},{"cell_type":"code","execution_count":null,"source":["# The RoBERTa model can also be used to classify if two sentences have similar or contradictory meanings\n","with torch.no_grad():\n","    inputs = ['I am running!', 'I am walking!']\n","    tokens = roberta.encode(*inputs)\n","    prediction = roberta.predict('mnli', tokens).argmax().item()\n","    print(prediction) # 0 means contradictory\n","\n","    inputs = ['I am running!', 'I run very fast!']\n","    tokens = roberta.encode(*inputs)\n","    prediction = roberta.predict('mnli', tokens).argmax().item()\n","    print(prediction) # 1 means similar"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00035-6a782035-ef13-456d-b9d8-aaf3e1ec6a19","output_cleared":false,"source_hash":"2acdff88","execution_millis":1121}},{"cell_type":"markdown","source":["### Exercises"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00036-e9cc9b2e-3e34-4031-b43b-3e618a23e67d","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Run horse2zebra on the dog image, we'll have to use the preprocess transforms for CycleGAN\n","preprocess = vis.transforms.Compose([\n","    vis.transforms.Resize(256),\n","    vis.transforms.ToTensor()\n","])\n","\n","img_url = 'https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dog.jpg'\n","response = requests.get(img_url)\n","img = pil.Image.open(BytesIO(response.content))\n","img_t = preprocess(img)\n","batch_t = torch.unsqueeze(img_t, 0)\n","\n","batch_out = netG(batch_t)\n","out_t = (batch_out.data.squeeze() + 1.0) / 2.0\n","vis.transforms.ToPILImage()(out_t)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00037-51be22c1-2e67-4df7-9226-e28ded8ba4e8","output_cleared":false,"source_hash":"defaa51","execution_millis":2242,"execution_start":1606271132853}},{"cell_type":"markdown","source":["Search to see how many files that contain `hubconf` on GitHub.\n","\n","Until now (2020-08-22), there are 932 files that includes the word `hubconf` on GitHub. \n","\n","Let's see the Google Trend on `Pytorch Hub`.\n","\n","<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/2213_RC01/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"pytorch hub\",\"geo\":\"\",\"time\":\"2004-01-01 2020-08-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=pytorch%20hub\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script>"],"metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00038-b2b26d33-46f4-4516-bfe5-355b937196f8","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Load the google trend til today \n","import datetime as dt\n","import pandas as pd\n","trend = pd.read_csv('https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/data/google_trend_pytorch_hub.csv', parse_dates=[0])\n","# Plot the trend\n","trend.plot(x='Month', y=1)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00039-49e9550f-fa2a-4098-98d4-090342e8213f","output_cleared":false,"source_hash":"ce4572cd","execution_millis":1300,"execution_start":1606271271269}},{"cell_type":"markdown","source":["The drop between the end of 2019 and the first half of 2020 is when the pandemic explodes."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00040-bed1fe89-b05d-48ac-9bd9-01d5832f4f72","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Let's see which countries top the trend\n","import pandas as pd\n","region = pd.read_csv('https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/data/google_trend_pytorch_hub_region.csv', header=0).fillna(0)\n","region.head(n=5)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00041-a88bf2df-f0a7-4d39-9a5e-719590393023","output_cleared":false,"source_hash":"183c6c16","execution_start":1606271283373,"execution_millis":97}},{"cell_type":"markdown","source":["## Chapter III: All as tensors\n","\n","Tensors are the numerical abstraction about anything with arbitrary dimensions, including the inputs, the intermediate representations in the network, and the output. Tensors are multidimensional, floating-point array.\n","\n","Deep learning may be viewed as numerical transformations of tensors.\n","\n","> ![dl_pytorch_c3_0](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_0.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","Tensors serve as representations for data with arbitrary dimensions.\n","\n","> ![dl_pytorch_c3_1](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_1.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","### Comparison between Pytorch tensors and NumPy arrays.\n","\n","* Both can represent data with arbitrary dimensions.\n","* Both support rich manipulations on the data and share almost the same APIs.\n","* Pytorch supports GPU accelerated operations directly on the tensors.\n","* Pytorch supports backtracing of the computational graph applied on the tensors.\n","* NumPy has exellent companion extension libraries such as SciPy, Scikit-learn, and Pandas.\n","* Pytorch tensors can be easily converted back and forth with NumPy arrays.\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00042-608b559c-ed65-4bc0-a180-b4c5ba3da2d4","output_cleared":false}},{"cell_type":"markdown","source":["### Basic operations and in-memory representation\n","\n","The operations on tensors always return another tensor. And the returned sensor points to the same memory location as the orignal tensor. This avoids data copying in the memory, which hugely improves computational performance. However, users have to be careful that the operations may change the underlying values."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00043-451ae363-e698-4437-b5e9-57220c837736","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["import torch\n","# Create 1 x 4 all-one tensor\n","a = torch.tensor(range(4)) + 1\n","print(a)\n","# Indexing\n","a[0]\n","# Change values via assignment, notice how the value of b has also been changed.\n","b = a[1]\n","a[1] = 0\n","print(a)\n","print(b)\n","# Check type of the data, indexing returns a 0-dimensional tensor a.k.a the scalar\n","print(type(a[1]))\n","# Convert the 0-dimensional tensor to a python number\n","print(float(a[1]))\n","# check the memory address\n","\n","# create another tensor in 2D\n","c = a.reshape(2, 2)\n","print(c.shape)\n","\n","# Truly copy the tensor\n","d = a.clone()\n","d[1] = 2\n","print(d)\n","print(a)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00044-5c709433-c12e-4a31-b78f-eccf9c006a19","output_cleared":false,"source_hash":"75caa53d","execution_start":1606271302904,"execution_millis":7}},{"cell_type":"markdown","source":["Tensors and NumPy arrays are contiguous byte blocks in memory, different from python list object, in which each item is stored separately in memory.\n","\n","> ![dl_pytorch_c3_2](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_2.png)\n","> \n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","Tensors are views of the underlying `torch.Storage` instances. Therefore, two tensors may share the same `Storage` instance while indexing to it in different ways.\n","\n","> ![dl_pytorch_c3_3](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_3.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00045-eda0203e-08bd-4931-ac57-dc659441b4a8","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# You may access the underlying storage instance by the Storage method\n","# The storage is regardless of the tensor's dimensions and always stores all data as a single 1-dimensional contiguous array.\n","print(a.storage())\n","print(b.storage())\n","print(c.storage())\n","print(d.storage())\n","# The storage is indexable\n","print(a.storage()[0])\n","print(c.storage()[0])"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00046-44de393d-985f-44ea-99b1-4652bf077a31","output_cleared":false,"source_hash":"6334e55e","execution_start":1606271314386,"execution_millis":0}},{"cell_type":"markdown","source":["Tensors use `size`, `offset`, and `stride` to define the indexing rules to the underlying storage instance.\n","\n","> ![dl_pytorch_c3_4](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_4.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","> \"The __size__ (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. The storage\n","__offset__ is the index in the storage corresponding to the first element in the tensor.\n","The __stride__ is the number of elements in the storage that need to be skipped over to\n","obtain the next element along each dimension.\"\n","\n","Most tensor shape or indexing transformations do not require memory reallocation of data, but simply math transformation on `size`, `offset`, and `stride`. \n","\n","For example, when applying transpose to a tensor, there will be no memory reallocation of data. Instead, the values in the stride tuple will be swapped.\n","\n","> ![dl_pytorch_c3_5](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_5.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00047-a439082a-136b-45ae-a7d9-0ea7457a2c60","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Check the different size, offset, and stride in two tensors with the same underlying storage instance\n","print(a.shape, a.storage_offset(), a.stride())\n","print(c.shape, c.storage_offset(), c.stride())"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00048-24f784fe-e39c-4838-86df-3488f31a1d90","output_cleared":false,"source_hash":"9736fd0c","execution_start":1606271325332,"execution_millis":4}},{"cell_type":"markdown","source":["### Name the tensor dimensions and dimension matching\n","\n","This is a handy way for humans to keep track the tensors as we go through a lot of transformations."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00049-4be118a4-cb99-489e-b2d9-8f6b4acefb00","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# A typical single RGB image tensor, 3 channels x 100 x 100\n","img_t = torch.randn(3, 5, 5)\n","weights = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels']) # These value will be applied to each channel\n","print(weights)\n","# A typical batch of RGB images tensor, 12 batches x 3 channels x 100 x 100\n","batch_t = torch.randn(2, 3, 5, 5)\n","# The index of the channel dimension is different in img_t and batch_t\n","# We better add consistent names to img_t and batch_t\n","img_t = img_t.refine_names('channels', 'width', 'height')\n","print(img_t.names)\n","batch_t = batch_t.refine_names(None, 'channels', 'width', 'height')\n","print(batch_t.names)\n","# Convert the RGB image to grayscale\n","# First make sure the weights tensor has the same dimension as img_t\n","weights_aligned = weights.align_as(img_t)\n","print(weights_aligned.shape, weights_aligned.names)\n","# Apply transformation\n","grayscale_t = (img_t * weights_aligned).sum('channels')\n","print(grayscale_t.shape, grayscale_t.names)\n","# Convert the grayscale to have the channel dimension\n","grayscale_t = grayscale_t.align_as(img_t)\n","print(grayscale_t.shape, grayscale_t.names)\n","\n","# Note that not all operations support named tensors, so you have to know how to convert it back to unnamed tensors\n","grayscale_t = grayscale_t.rename(None)\n","print(grayscale_t.shape, grayscale_t.names)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00050-2fa47292-d6f2-4998-a3d1-e043a15349e7","output_cleared":false,"source_hash":"151f132b","execution_start":1606271329592,"execution_millis":7}},{"cell_type":"markdown","source":["### Tensor data types and impact on model performance\n","\n","Why pytorch needs dedicated data types?\n","\n","* Numbers in Python are object, occupying more memory than floating numbers.\n","* Lists in Python are collections of objects, not just numbers.\n","* Python is an interpreter and runs slow on math operations.\n","\n","Pytorch supports floating and integer data types as NumPy supported. While most modern CPU uses at least 32-bit floating point numbers, modern GPUs can support 16-bit floating point numbers. Integer tensors will always be created with `int64` type, this can support large scale indexing for tensors with millions of elements.\n","\n","The performance impacted by data types.\n","\n","* 64-bit floating point numbers do not really improve the performance over 32-bit floating point numbers.\n","* 16-bit floating point numbers can cut model size into half with slight sacrifice in the performance."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00051-f751a5d1-72fa-46aa-85ea-00d2fa3a91c3","output_cleared":false}},{"cell_type":"markdown","source":["### Contiguous tensors\n","\n","This is something that I'm now getting it. Here's some explanation from the PyTorch forum.\n","\n","> Contiguous is the term used to indicate that the memory layout of a tensor does not align with its advertised meta-data or shape information.\n","> \n","> -- https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/7\n","\n","---\n","\n","This basically means the order of each data in the memory matches the order of the meta-data of the tensor (size, offset, stride). So a contiguous tensor typically is the tensor when being first created and has not been applied any transformations that change the values of stride, size or offset."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00052-d53586c4-cae4-4a80-9efd-83bb5dd7bc2b","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["### Check contiguous status\n","print(a.is_contiguous())\n","print(b.is_contiguous())\n","print(c.is_contiguous())\n","print(d.is_contiguous())\n","# transpose \n","e = c.transpose(0, 1)\n","print(c.stride())\n","print(e.stride())\n","print(e.is_contiguous())"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00053-337c0c96-6d2b-47c2-b924-1aa15e6877cf","output_cleared":false,"source_hash":"6ae8ab9f","execution_start":1606271336135,"execution_millis":3}},{"cell_type":"markdown","source":["### Interoperability on GPU, with NumPy\n","\n","With GPU, all operations will be on GPU, but accessing and printing will wake up CPU. All APIs are available for gpu-enabled tensors. And the storage instance of the cpu tensors will be copied to the RAM of the GPU.\n","\n","With NumPy, the storage instance will be the same for NumPy array and the tensor. Note that the floating point number in NumPy array is 64-bit, so after converting back from NumPy, **make sure we cast the tensor to `torch.float32` to save computational power**."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00054-4221d11d-43fe-4dd3-afe4-3d797110d6ec","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# To GPU, no GPU available so error raised\n","test = torch.rand(3)\n","try:    \n","    test.to(device='cuda:0')\n","except AssertionError as e:\n","    print(e)\n","\n","# To NumPy\n","test_np = test.numpy()\n","print(test_np.dtype)\n","\n","# From NumPy\n","import numpy as np\n","test_np = np.random.rand(3)\n","print(test_np.dtype)\n","test = torch.from_numpy(test_np)\n","print(test.dtype)\n","test = test.to(dtype=torch.float32)\n","print(test.dtype)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00055-857dc320-9b73-495b-91c6-bcfea7bed2c3","output_cleared":false,"source_hash":"d62ce2a8","execution_start":1606271339500,"execution_millis":20}},{"cell_type":"markdown","source":["PyTorch uses dispatching mechanism to make the operations are interoperational over data stored on different backends.\n","\n","> ![dl_pytorch_c3_6](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c3_6.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\""],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00056-c12e085d-0dfa-4227-900d-afa29316a6ba","output_cleared":false}},{"cell_type":"markdown","source":["### Conditional indexing on tensors\n","\n","We may use bool tensor as indices to index the tensors."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00057-76fce046-73ff-434f-a1ea-3d8e11da56b5","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Create a tensor \n","a = torch.tensor([[1, 2, 3], [4, 5, 6]]).float()\n","print(a.shape, a.dtype)\n","# Indexing the the first row using bool array\n","mask = torch.tensor([1, 0]).bool()\n","print(mask.shape, mask.dtype)\n","first_row = a[mask, :]\n","print(first_row)\n","\n","# Create bool array using comparison operators\n","mask2 = a < 3\n","a_lt_2 = a[mask2]\n","print(a_lt_2)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00058-be6c14e3-074a-4a53-93b5-7aae6dab0ca1","output_cleared":false,"source_hash":"b78a125e","execution_start":1606271360617,"execution_millis":1}},{"cell_type":"markdown","source":["### Serializing tensors\n","\n","Serializing could be to pickle files, to hdf5 format, or to csv readable format.\n","\n","> HDF5 is a portable, widely supported\n","format for representing serialized multidimensional arrays, organized in a nested keyvalue\n","dictionary."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00059-d3a68296-0fda-4a86-9114-7864eaac2291","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Save as pickle\n","torch.save(a, 'a.t')\n","# Load pickle\n","a = torch.load('a.t')\n","print(a)\n","os.remove('a.t')"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00060-df39c925-df28-4c96-90c7-defb83153aa3","output_cleared":false,"source_hash":"cfa7e9a5","execution_millis":5,"execution_start":1606271411812}},{"cell_type":"code","execution_count":null,"source":["# Save as hdf5\n","import h5py\n","with h5py.File('a.hdf5', 'w') as f:\n","    dset = f.create_dataset('test', data=a.numpy())\n","\n","# Load hdf5\n","with h5py.File('a.hdf5', 'r') as f:  \n","    test_h5 = f['test']\n","    print(torch.from_numpy(test_h5[:]))\n","os.remove('a.hdf5')"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00061-bbe760a5-cf52-4c12-99a8-dd37c6b3e4b7","output_cleared":false,"source_hash":"7d8e34b4","execution_millis":4,"execution_start":1606271437360}},{"cell_type":"markdown","source":["### Excercise"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00062-6495ecbc-af9b-48b4-bb08-a38746add815","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# cos needs float pointing number\n","a_32 = a.to(torch.float32)\n","print(a_32.cos())\n","print(a_32.sqrt())\n","print(a_32.storage())\n","# inplace\n","torch.cos(a_32, out=a_32)\n","print(a_32.storage())\n","# or\n","a_32.cos_()\n","print(a_32.storage())"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00063-d8b5f385-dff4-452d-ac8f-25cfe108925d","output_cleared":false,"source_hash":"32a97f5","execution_start":1606271451256,"execution_millis":13}},{"cell_type":"markdown","source":["## Chapter IV: Turn real-world data into tensors\n","\n","### Dealing with images\n","\n","Convert RGB images as tensors. Note how the dimensions in pytorch are formed as `channels x height x width`. But in tensorflow, the dimensions are by default formed as `height x width x channels`."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00064-22866c01-4b7d-4e7a-ae42-62777b06bf8a","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Read in an image\n","import imageio\n","import torch\n","img = imageio.imread('https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dog.jpg')\n","# shape is in height x width x channels\n","print(img.shape)\n","# pytorch tensors should have the dimensions formed as channels x height x width, so we need to rearrange the dimensions\n","img_t = torch.from_numpy(img)\n","print(img_t.shape)\n","# rearrange\n","img_t = img_t.permute(2, 0, 1)\n","print(img_t.shape)\n","# Put the image in a batch by adding a new dimension at dimension 0, for pytorch it becomes batch x channels x height x width\n","batch_t = img_t.unsqueeze(0)\n","batch_t = batch_t.expand(5, -1, -1, -1)\n","print(batch_t.shape)\n","# Or create a batch_t at first and then copy the image to the batch\n","batch_t = torch.zeros(5, 3, 183, 275)\n","batch_t[0] = img_t\n","print(batch_t.shape)\n","# Images by default are stored as unsigned 8-bit integers. \n","print(batch_t[0, 0, 0, 0].dtype)\n","# We need to convert them into floating point numbers.\n","batch_t = batch_t.float()\n","print(batch_t[0, 0, 0, 0].dtype)\n","# And better normalize them to [0, 1] or [-1, 1] which is required for efficient optimization when training the neural network\n","# Normalize them to [0, 1]\n","batch_t = batch_t / 255.0\n","# Standardize them to have zero mean and unit standard variation\n","# The mean is computed for each channel across the batch\n","mean_c_t = batch_t.mean([0, 2, 3]).unsqueeze(1).unsqueeze(1)\n","std_c_t = batch_t.std([0, 2, 3]).unsqueeze(1).unsqueeze(1)\n","print(mean_c_t.shape)\n","print(std_c_t.shape)\n","# Using broadcasting semantics, the trailing singleton dimensions (height, width) will be expanded automatically to do pointwise operations \n","n_batch_t = (batch_t - mean_c_t) / std_c_t\n","print(n_batch_t.mean([0, 2, 3])) # Close to 0, because this is numerical\n","print(n_batch_t.std([0, 2, 3]))"],"outputs":[],"metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00065-94b98a19-ffbb-4a4c-9f0f-736edb490152","output_cleared":false,"source_hash":"75dfc71f","execution_millis":110,"execution_start":1606271513109}},{"cell_type":"markdown","source":["### Dealing with 3D images or videos\n","\n","In addition to channels, 3D images and videos will have one more dimension for depth, and for time. Therefore, the shape of a batch of 3D images would be: `batch x channel x depth x height x width`; while the shape of a batch of videos would be: `batch x channel x time x height x width`. \n","\n","Example of 3D CT images\n","\n","> ![dl_pytorch_c4_1](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c4_1.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","\n","Related APIs and resources to load and transform videos in PyTorch.\n","\n","1. [torchvision.io.video](https://pytorch.org/docs/1.3.0/torchvision/io.html#video)\n","2. [pytorch-VideoDataset](https://github.com/YuxinZhaozyx/pytorch-VideoDataset)\n","3. [torch_videovision](https://github.com/hassony2/torch_videovision)"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00066-4846d97f-c247-42ab-ad9f-3d5cc57e662d","output_cleared":false}},{"cell_type":"markdown","source":["### Dealing with tabular data\n","\n","Pytorch tensors are homogeneous, but tabular data may include categorical, atomic data, which need to be converted to numerical data, usually with One-hot encoding.\n","\n","By convention, the dimension of tabular data in PyTorch may be represented as `batch x channel`."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00067-e2128eb3-6662-4adb-806d-62de91cbdcb1","output_cleared":false}},{"cell_type":"markdown","source":["1. Continuous values: as real numbers, strictly ordered, strict meaning in the difference between values. Support mathematical operations on the values.\n","    1. Ratio scale: the difference between values can be computed as the ratio of the values.\n","    2. Interval scale: the difference between values can only be computed as interval between values.\n","2. Ordinal values: as real integers, strictly ordered, but no fixed meaning in the difference between values. Do not support mathematical operations on the values but just ordering operations.\n","3. Categorical values: as real integers, no order, no fixed meaning in the difference between values. Do not support mathematical and ordering operations on the values. They are on nominal scale."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00068-83a55325-de3e-4a70-b639-5a5bd8579e1a","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["# Load the wine quality tabular dataset\n","# !wget http://mng.bz/90Ol --no-check-certificate -O wine.csv\n","    \n","import pandas as pd\n","import numpy as np\n","wineq_pd = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', dtype=np.float32, delimiter=\";\",\n","skiprows=1)\n","wineq_numpy = wineq_pd.values\n","wineq = torch.from_numpy(wineq_numpy)\n","wineq.size()"],"outputs":[],"metadata":{"deepnote_cell_type":"code","scrolled":false,"cell_id":"00069-ca02088e-a4fe-4f3d-b796-ed1968e9ee78","output_cleared":false,"source_hash":"dff71e4a","execution_millis":612,"execution_start":1606271667349}},{"cell_type":"code","execution_count":null,"source":["X = wineq[:, :-1]\n","y = wineq[:, -1].long()\n","X.shape, y.shape, X.dtype, y.dtype"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00070-6ce060dc-bc5a-4247-976b-1a76283f4e9f","output_cleared":false,"source_hash":"7aa61489","execution_start":1606271672286,"execution_millis":4}},{"cell_type":"markdown","source":["One hot encoding on `y` if we want to use it as model input, otherwise we may use it directly as the target of the model.\n","\n","Note the size of the onehot encoded array should be long enough to include the max of `y` (sometimes `y` does not start from 0)."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00071-5c23d78e-78ba-4a6f-ab94-06b4e3ca0f0d","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["encoded_y = torch.zeros(y.shape[0], y.max() + 1)\n","print(y.unique())\n","encoded_y.scatter_(1, y.unsqueeze(1), 1.0)\n","encoded_y.shape, encoded_y.dtype"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00072-b58b8e34-70a4-4249-8184-96c8cddced95","output_cleared":false,"source_hash":"8a5d2591","execution_start":1606271675649,"execution_millis":1}},{"cell_type":"markdown","source":["### Dealing with time series data\n","\n","1. Time series tabular data\n","\n","By convention, the dimensions of time series tabular data would typically be `batch x channel x time`, where each channel represents one feature.\n","\n","2. Audio or sensor data\n","\n","By convention, the dimensions of audio or sensor data (represented in spectrogram) would typically be `batch x channel x freq x time`, where channel is the \"audio channel (e.g., left and right) or \"axial channel\" (e.g., x, y, and z for multiaxial sensors) and `freq x time` look like an image. Therefore, networks used for images may be used for audio tensors as well.\n","\n","Some good tools and resources to deal with time series data in PyTorch.\n","\n","1. [torchaudio](https://pytorch.org/audio/)\n","2. [pytorch-ts](https://github.com/zalandoresearch/pytorch-ts)"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00073-d50207d3-5634-4caa-9c9c-219303476b7e","output_cleared":false}},{"cell_type":"markdown","source":["### Dealing with text\n","\n","Text needs to be converted to numbers, using embedding."],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00074-71876c80-addf-4c92-8811-30a0ac8fca6c","output_cleared":false}},{"cell_type":"code","execution_count":null,"source":["import re\n","response = requests.get('https://www.gutenberg.org/files/1342/1342-0.txt')\n","text = BytesIO(response.content).getvalue().decode('utf-8')\n","text = text.lower().strip()\n","tokens = re.split('\\n+', text)\n","print(tokens[1:5])\n","print(len(tokens), len(np.unique(tokens)))"],"outputs":[],"metadata":{"deepnote_cell_type":"code","scrolled":false,"cell_id":"00076-c662a960-1689-46fb-9c78-7ed3e9e97ab3","output_cleared":false,"source_hash":"656bb6e0","execution_millis":340,"execution_start":1606271770094}},{"cell_type":"code","execution_count":null,"source":["# Apply onehot encoding to each character for each line with ASCII encoding\n","\n","# 128 ASCII characters\n","onehot_tokens = []\n","for token in tokens:\n","    token = token.lower().strip()\n","    onehot_token = torch.zeros(len(token), 128)\n","    # Use ord to get ASCII code for the character\n","    for i, c in enumerate(token):\n","        if ord(c) < 128:\n","            onehot_token[i][ord(c)] = 1\n","    onehot_tokens.append(onehot_token)"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00077-7145d7bc-0f5b-437d-b9ad-cfba8e58aaac","output_cleared":false,"source_hash":"a47e3d96","execution_start":1606271753889,"execution_millis":6933}},{"cell_type":"code","execution_count":null,"source":["# Apply onehot encoding to each word"],"outputs":[],"metadata":{"deepnote_cell_type":"code","cell_id":"00078-1866c471-c6a6-49af-9706-a2798099c7cf","output_cleared":false,"source_hash":"74a297bf"}},{"cell_type":"markdown","source":["### Encoding schemes\n","\n","Encoding is needed for categorical data. Depending on whether ordering matters or not, the categorical data may be encoded as continuous values or onehot coded integers. More than that, variables may be encoded via **embedding** that converts categorical relational data (such as words) into numerical vectors, and use the distances between the encoded vectors to measure the similarity between the categorical data.\n","\n","Here's the workflow for encoding different types of data as tensors for pytorch models, embedding is not included.\n","\n","> ![dl_pytorch_c4_2](https://raw.githubusercontent.com/digitalideation/digcre_h2201/master/samples/images/dl_pytorch_c4_2.png)\n",">\n","> -- From the book \"Deep Learning with Pytorch\"\n","---"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00079-f20d7f8d-1f13-40c2-9ae2-98cecd018e86","output_cleared":false}},{"cell_type":"markdown","source":["**The curated list related to PyTorch.** https://www.ritchieng.com/the-incredible-pytorch/"],"metadata":{"deepnote_cell_type":"markdown","cell_id":"00080-d9028c3e-0dc0-46f1-8773-eea22f60f2ad","output_cleared":false}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"deepnote_notebook_id":"f2d65cd7-8683-43b5-980d-c0fcc232bbda","deepnote_execution_queue":[]}}